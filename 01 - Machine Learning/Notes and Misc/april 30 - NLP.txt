NLP - Natural Language Processing

Preprocessing
1.Remove HTML tags
	when we are webscrapping, HTML tags should be removed
2.Remove extra whitespaces
3.Convert accented characters to ASCII characters
4.Expand contractions and abbrevations
5.Remove special characters 
6.#Remove stopwords      
7.#Lowercase all texts
8.#Lemmatization or Stemming - only 1 is required in a single usecase
	this is the process of identiying the root word for the actual word.
	Lemmatization - programatically trimming the tail part of the word.
		Eg:	walking - walk
			bought - bough
			thought - though
	Stemming - Identifying the actual root word
		Eg:	walking - walk
			bought - buy
			thought - think
9.#Tokenizing
	Split the data(sentence) into tokens
10.Convert number words to numeric form
	10 Crore - 1,000,00,000
11.Vectorization
	Converting the text data into numerical vector.


Vectorization
However, when doing natural language processing, words must be converted into vectors that machine learning algorithms can make use of. 
If your goal is to do machine learning on text data, like movie reviews or tweets or anything else, you need to convert the text data into numbers. 
This process is sometimes referred to as “embedding” or “vectorization”.

Techniques for vectorization
1.TFIDF
2.Word2vec (google apis)

TFIDF - Term Frequency - Inverse Document Frequency

In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf.


TFIDF = TF*log(1/DF)

DF - number of documnets contains the word/total documents

for IDF,we  are taking the inverse of it .

IDF = total documents/number of documnets contains the word
and taknig the log(IDF)

s1 . earth is the third planet from the sun
s2 . jupiter is the largest planet

words	TF(s1)	TF(s2)	IDF(log(TD/AD))	TFIDF(s1)		TFIDF(s2)

earth	1/8	0	log(2/1)=0.3	0.0375		0
is	1/8	1/5	log(2/2)=0	0		0
the	2/8	1/5	log(2/2)=0	0		0
third	1/8	0	log(2/1)=0.3	0.0375		0
planet	1/8	1/5	log(2/2)=0	0		0
from	1/8	0	log(2/1)=0.3	0.0375		0
sun	1/8	0	log(2/1)=0.3	0.0375		0
jupiter	0	1/5	log(2/1)=0.3	0		0.06
largest	0	1/5	log(2/1)=0.3	0		0.06



vector output
Rows - no of observations in input
Columns - TFIDF value of each word

index	earth	is	the	 third	planet	from	sun	jupiter	largest	
s1	0.0375	0	0	0.0375	0	0.0375	0.0375	0	0
s2	0	0	0	0	0	0	0	0.06	0.06


for NLP preprocecssnig - NLTK is used (Natural Language ToolKit - A python package)
we have funcitons for 
	STOPWORDS
	Lemmatization
	Stemming
	TFIDF vectorizor

for installation: pip install nltk


