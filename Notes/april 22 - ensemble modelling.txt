
Ensemble modeling is a process where multiple diverse base models are used to predict an outcome. 

Bagging and Boosting

Bagging -  the objective is to create several subsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train their decision trees.
Bagging is a paralellel way of ensembling.

Bagging Steps:

	1.Suppose there are N observations and M features in training data set. A sample from training data set is taken randomly with replacement.
	2.A subset of M features are selected randomly and whichever feature gives the best split is used to split the node iteratively.
	3.Above steps are repeated n times and prediction is given based on the aggregation of predictions from n number of trees.

Advantages:

	1.Reduces over-fitting of the model - [we dont use entire features together]
	2.Handles higher dimensionality data very well -[we dont use entire features together]	
	3.Maintains accuracy for missing data.




Boosting -In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analysing data for errors. Consecutive trees (random sample) are fit and at every step, the goal is to improve the accuracy from the prior tree.

 
Random Forest ? 

Random Forest - Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.


What is Entropy - How the nodes in decision trees are decided..


Entropy is the measures of disorder or uncertainty in a bunch of examples.

Entropy controls how a Decision Tree decides to split the data. It actually effects how a Decision Tree draws its boundaries.

Entropy = - Sigma[Pi*log2(Pi)] = -(P1logP1+P2logP2+........+PnlogPn)

Information gain (IG) measures how much “information” a feature gives us about the class.
IG is the main key that is used by Decision Tree Algorithms to construct a Decision Tree.
Decision Trees algorithm will always tries to maximize Information gain.
An attribute with highest Information gain will tested/split first.

IG=Entropy(parent) - (weights average)*Entropy(children)



Grade		Bumpiness		Speed Limit		Speed(op class)
steep		bumpiness		yes			slow
steep		smooth			yes			slow
flat		bumpiness		no			fast
steep		smooth			no			fast


Parent Class - Output Class
P1 = P(slow) = 0.5
P2 = P(fast) = 0.5
Entropy of Parent Class = -(.5*log.5+.5*log.5) = 1

Grade		class	count	slow	fast	Entropy			Weight Average
Left Hand		steep	3	2	1	-(2/3log2/3+1/3log1/3) = 0.9	3/4
Right Hand	flat	1	0	1	-(1log1) = 0		1/4
weighted average * Entropy(childred) 	= 	3/4*0.9+1/4*0 = 0.675
IG(Grade) = 1-0.675 = 0.325


SpeedLimit	class	count	slow	fast	Entropy			Weight Average
Left Hand		yes	2	2	0	-(2/2log2/2) = 0		2/4
Right Hand	no	2	0	2	-(2/2log2/2) = 0		2/4
weighted average * Entropy(SpeedLimit) 	= 	2/4*0+2/4*0 = 0
IG(SpeedLimit) = 1-0 = 1

Bumpiness	class		count	slow	fast	Entropy			Weight Average
Left Hand		bumpiness	2	1	1	-2*(1/2log1/2) = 1		2/4
Right Hand	smooth		2	1	1	-2*(1/2log1/2) = 1		2/4
weighted average * Entropy(Bumpiness) 	= 	2/4*1+2/4*1 = 1
IG(Bumpiness) = 1-1 = 0


As we know that, Decision Tree Algorithm construct Decision Tree based on features that have highest Information gain.
So, here we can see that SpeedLimit has highest Information gain.So root node will be SpeedLimit



sample data for calculating Entropy and IG

Fruit Dataset

Loan Approval Dataset

SalaryClass	RepaymentStatus		OwnedHouse	CibilScoreClass	LoanStatus(op class)
<50K		Satisfactory		Yes		bad		Rejected	
<50K		Satisfactory		No		bad		Rejected
>50K		NonSatisfactory		No		bad		Rejected
<50K		Satisfactory		Yes		good		Approved
>50K		NonSatisfactory		Yes		good		Approved
<50K		Satisfactory		No		bad		Rejected	


please calculate the entropy and IG and identify the root node.

Parent Class - LoanStatus
Entropy(Parent) = -(4/6log4/6+2/6log2/6) = 0.272
SalaryClass (is salary <50K)
	total	rejected	approved	entropy			WeightedAvereage
LH(Yes)	4	3	1	-(3/4log3/4+1/4log1/4)	4/6
RH(No)	2	1	1	-(1/2log1/2+1/2log1/2)	2/6






	