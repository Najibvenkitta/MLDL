
User Defined function for Entropy

entropy = -(Pi*logPi)
input will be a list
output will be a floaing value of entropy
math package - math.log2 function

sample input		sample ouput
[1,1]			1
[3,1]			0.811
[3,3]			1
[5,7]			0.98

def entropy(X): #X is the list of counts of classes
	total = sum(X)
	entropy=0
	for x in X:
		entropy = entropy+(x/total)*math.log2(x/total)
	entropy=entropy*-1
	return(entropy)


IG is the criteria for selecting nodes in decision tree. Similarly there is one more criteria called ''gini index''.
Please explore what is gini index. 



Classification model evaluation parameter - extended

Precision
Recall
F1-Score

Accuracy  = TP+TN/total Results
Precision = True Positive / Actual Results = TP/(TP+FP) 
Recall = True Positive/Prediced Results = TP/(TP+FN)
Specificity - Same as Recall for Negative  = TN/(TN+FP)
F1-Score = 2*P*R/(P+R)

ROC-AUC Curve

AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.






