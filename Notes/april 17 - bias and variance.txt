Bias and Variance

There are 2 error can be calculated.
1.Testing Error/Evaluation error
	Ytest and Ypred

2.Training error - we can predict the result for Xtrain which is already used for training.
	model.fit(X_train,y_train)
	ytrainpred=model.predict(X_train)
	MSE(ytrainpred,y_train)enna 
	

ideal situation - best model
	Low Training Error and Low Evaluation Error

Other situations
	Low Training Error and High Evalutaion Error
		- This situation is known as Overfitting.

	High Training Error and High Evalution Error
		-underfitting
		- Change the algorithm
		- Use more data field/data

Practical Example
A maths exam topics are 
	Integration
	Trignometry
	Probability
	Algebra

I learnt only integration-out of 100 marks - max of 25 marks - this is an example for overfitting
	If only integration = 100 marks 

what if I have learnt basic of everything - 20 marks - this is an example for underfitting 
	due to lack of avialbality of data
	wrong model selection

what if i have learnt every chapter in an average way - 15*4 = 60 
	this is an example for right fit. if only integration = 60



What is Bias - Bias is the difference between the average prediction of our model 
	and the correct value which we are trying to predict. (nothing but the Error)


What is Variance - Variance is the variability of model prediction for a given data point or a value 
	which tells us spread of our data.


a best model will have Low Bias(low training error) and Low Variance(low testing error)


if bias is low - low Training Error
if bias is high - high training error
if variance is low - EE is same as TE
if variance is high - EE is not same as TE


					Bias
				Low		High

		Low	low TE,low EE		high TE,high EE		

Variance

		High	low TE,High EE		Not Exisiting(high TE,low EE)



Right Model - Low Bias,Low Variance
Underfit - High Bias,Low Variance
Overfit - Low Bias, High Variance



How to handle underfitting
	Use better algorithm
	Use more data

How to handle Overfitting
	(we have enough data and the model is also good)
	There are many techniques to avoid/minimize overfitting
	1.Regularization - mostly used for linear models
		a.Ridge Regression
		b.Lasso Regression
		c.Elastic Net
	2.Ensemble Models - used for nonliner models
		a.Bagging - Random Forest algorithm
		b.Boosting - XG Boost algorithm,Ada boost algorithm